<!doctype html><html lang=en><head><title>The Role of Covariance in Principal Component Analysis · Home
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Hieu Nguyen"><meta name=description content="I recently read an interesting explanation for covariance on StackExchange Cross Validated. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don&rsquo;t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="The Role of Covariance in Principal Component Analysis"><meta name=twitter:description content="I recently read an interesting explanation for covariance on StackExchange Cross Validated. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don’t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method."><meta property="og:url" content="https://quanghieu31.github.io/posts/pca-covariance/"><meta property="og:site_name" content="Home"><meta property="og:title" content="The Role of Covariance in Principal Component Analysis"><meta property="og:description" content="I recently read an interesting explanation for covariance on StackExchange Cross Validated. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don’t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-10T12:00:00+00:00"><meta property="article:modified_time" content="2024-09-10T12:00:00+00:00"><link rel=canonical href=https://quanghieu31.github.io/posts/pca-covariance/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.38c4552ac40f9ae3408bad40358f654ebd8804412fe74ed56f2d6c8a7af82dd3.css integrity="sha256-OMRVKsQPmuNAi61ANY9lTr2IBEEv507Vby1sinr4LdM=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/custom.min.82caf190d01cd5b664127e60afa93081244659afd1e288eda818c81fb0e63981.css integrity="sha256-gsrxkNAc1bZkEn5gr6kwgSRGWa/R4ojtqBjIH7DmOYE=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon.ico sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Hieu Nguyen"><meta name=description content="I recently read an interesting explanation for covariance on StackExchange Cross Validated. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don&rsquo;t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="The Role of Covariance in Principal Component Analysis"><meta name=twitter:description content="I recently read an interesting explanation for covariance on StackExchange Cross Validated. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don’t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method."><meta property="og:url" content="https://quanghieu31.github.io/posts/pca-covariance/"><meta property="og:site_name" content="Home"><meta property="og:title" content="The Role of Covariance in Principal Component Analysis"><meta property="og:description" content="I recently read an interesting explanation for covariance on StackExchange Cross Validated. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don’t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-10T12:00:00+00:00"><meta property="article:modified_time" content="2024-09-10T12:00:00+00:00"><link href="https://fonts.googleapis.com/css2?family=Source+Sans+3&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/ style=font-size:18px>Home
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list style=font-size:18px><li class=navigation-item><a class=navigation-link href=/about/ style=font-size:18px>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/ style=font-size:18px>Posts</a></li></ul></section></nav><div class=content><section class="container post"><article><style>.content{max-width:1000px;margin:40px auto 10px;padding:0 15px;font-size:16px;line-height:1.8}</style><header><div class=post-title><h1 class=title><a class=title-link href=https://quanghieu31.github.io/posts/pca-covariance/ style=font-size:22px>The Role of Covariance in Principal Component Analysis</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2024-09-10T12:00:00Z>September 10, 2024
</time></span><span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
9-minute read</span></div></div></header><div class=post-content><p>I recently read an interesting explanation for covariance on <a href=https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean class=external-link target=_blank rel=noopener>StackExchange Cross Validated</a>. The usage of rectangles and colors is very intuitive to see the directions, means, and strengths given information on covariance and correlation. It brings me to another (not very relevant) thought: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don&rsquo;t I try implementing this by exploring the use of covariance matrix in this very popular dimension reduction/variable selection method.</p><p><strong>Note</strong>: If you think I make any mistakes or unclear points, please let me know via email and we can discuss it, I really appreciate your help!</p><h3 id=quick-on-pca--svd>Quick on PCA + SVD
<a class=heading-link href=#quick-on-pca--svd><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>SVD = Singular Value Decomposition</p><p>There seems to be many ways to construct PCA, here I am investigating sequentially the use of covariance matrix and then the use of SVD. A member of the basic linear transforms family, PCA+SVD is feature construction with the goal to transform data into a new coordinate system for dimensionality reduction or class separation. Basically, it reduces the number of features by getting linearly uncorrelated variables (or principal components, or eigenvectors) while maintaining variance. These variables belong to the left singular matrix of the SVD matrix while the variances are the middle matrix or diagonal matrix.</p><p>Principal components (PCs) are created (newly) from the data. Basically, PCA tries to convert the available into PCs that easier to read, contain useful information like variance and eigenvalues, but hard to be meaningfully interpreted because the measures or units are gone. Utilizing linear transforms, PCA finds weights as eigenvectors that maximizes variance or $||\mathbf{Xw}||^2$ (squared norm or squared length) of the projected data and thus gets these components through eigen-decomposition of the covariance matrix of the data. The resulting eigenvectors or PCs define the directions of maximum variance and eigenvalues quantify the amount of the PC&rsquo;s variance.</p><p>Note that these PCs are not features! Each PC is a direction of the data that is orthogonal to each other (no PCs or eigenvectors are overlapping) and represents the highest variance possible. For example, after finding the first PC with maximum variance, it essentially removes this PC from data, and find the next highest-variance PC. So, each PC contains &ldquo;portions&rdquo; of all features in original data (<a href=https://quanghieu31.github.io/posts/pca-covariance/#bonus-pca class=external-link target=_blank rel=noopener>example</a>).</p><p>There are so many details on this supervised feature selection method (e.g. the relevance of nested subset methods, Gram-Schmidt orthogonalization,&mldr;) but I mainly try to see how relevant the covariance matrix is.</p><p>Very helpful info on SVD: <a href=https://en.wikipedia.org/wiki/Singular_value_decomposition class=external-link target=_blank rel=noopener>https://en.wikipedia.org/wiki/Singular_value_decomposition</a> and PCA: <a href=https://en.wikipedia.org/wiki/Principal_component_analysis class=external-link target=_blank rel=noopener>https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><h3 id=1-covariance-matrix>1. Covariance matrix
<a class=heading-link href=#1-covariance-matrix><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Suppose we have a data matrix called $\mathbf{X}$ that is standardized and centered to the mean 0 (for each column). And suppose it only has continous, numerical variables for convenience. $\mathbf{X}$ is $n \times p$ where $n$ is the number of observations and $p$ is the number of features. Now, the covariance matrix is</p><p>$$
\mathbf{C} = \frac{1}{n-1} (\mathbf{X}^T-\bar{\mathbf{X}^T}) (\mathbf{X}-\bar{\mathbf{X}}) = \frac{1}{n-1} (\mathbf{X}^T) (\mathbf{X})
$$</p><p>The question now is why? Why does it look like this? Note that $\mathbf{X}$ contains multiple features/variables, so doing this means we simultaneously calculate the multiple cases of $Cov(\mathbf{X}, \mathbf{Y}) = \frac{1}{n-1} (\mathbf{X}-\bar{\mathbf{X}})(\mathbf{Y}-\bar{\mathbf{Y}})$. How to interpret the result of the covariance matrix? Each column and each row are the features themselves, the diagonal entries are the &ldquo;self&rdquo;-covariance while the rest show the covariance value between the intersected variables.</p><p>$\mathbf{C}$ is symmetric matrix and so it is diagonalizable: There exists a real orthogonal matrix $\mathbf{Q}$ and diagonal matrix $\mathbf{D}$ such that $$\mathbf{C} = \mathbf{Q}\mathbf{D}\mathbf{Q^T}$$</p><ul><li>$\mathbf{Q}$ contains all the eigenvectors of $\mathbf{C}$ which are the &ldquo;principal directions&rdquo;, showing the direction in the feature space along which the data varies the most.</li><li>$\mathbf{D}$ represents the eigenvalues or how much variance along that corresponding principal direction.</li></ul><p>Proof: <a href=https://en.wikipedia.org/wiki/Symmetric_matrix class=external-link target=_blank rel=noopener>https://en.wikipedia.org/wiki/Symmetric_matrix</a></p><h3 id=2-transformed-x>2. Transformed X
<a class=heading-link href=#2-transformed-x><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Projections of the data on the principal directions are called principal components, or PC scores; these can be seen as newly cretaed or transformed variables. So the transformed data is actually $\mathbf{X}\cdot\mathbf{Q}$</p><h3 id=3-svd>3. SVD
<a class=heading-link href=#3-svd><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>SVD factorizes $\mathbf{X}$ (not just square matrices) into three matrices (<a href=https://gregorygundersen.com/blog/2018/12/20/svd-proof/ class=external-link target=_blank rel=noopener>proof</a>):</p><p>$$
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
$$</p><p>where:</p><ul><li>$\mathbf{U}$ is a matrix of left singular vectors (related to the rows of $\mathbf{X}$),</li><li>$\mathbf{\Sigma}$ is a diagonal matrix of singular values (related to the square roots of the eigenvalues), the diagnonal entries, $\sigma_i = \Sigma_{ii}$ are uniquely defined by $X$ and known as the singular values of $X$. These are non-negative real numbers. by convention the diagonal elements are in descending order.</li><li>$\mathbf{V}^T$ is the transpose of a matrix of right singular vectors (related to the columns of $\mathbf{X}$).</li></ul><h3 id=4-covariance-matrix-and-svd>4. Covariance matrix and SVD
<a class=heading-link href=#4-covariance-matrix-and-svd><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Note that</p><p>$$
\mathbf{C} = \frac{1}{n-1}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T)^T \cdot (\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T) = \mathbf{V} \frac{\mathbf{\Sigma}^2}{n-1} \mathbf{V}^T
$$</p><p>From the breakdown in part (1), it seems that $\mathbf{Q}$ is actually $\mathbf{V}$! So $V$ contains eigenvectors or principal directions of the covariance matrix $\mathbf{C}$. And $\frac{\mathbf{\Sigma}^2}{n-1}=\mathbf{D}$ represents how strong the variance of the corresponding directions are (the diagonal entries are eigenvalues). And the transformed data is $\mathbf{X}\cdot\mathbf{V} = (n\times p) \cdot (p\times p)$.</p><p>The $i$-th column of $\mathbf{X}\mathbf{V}$: This column represents the values of the $i$-th principal component for each sample. It is the transformed version of the original data in the direction of the $i$-th eigenvector.</p><p>Also, notice that $\mathbf{X}\mathbf{V}=U\Sigma V^T V=U\Sigma$.</p><h3 id=5-reduce-dimensionality>5. Reduce dimensionality
<a class=heading-link href=#5-reduce-dimensionality><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Now, we would to have fewer features, i.e. $k&lt;p$.</p><ul><li>Get the initial $k$ columns from $\mathbf{U}$ or $n \times k$ matrix</li><li>Get the initial $k$ columns and $k$ rows from $\mathbf{\Sigma}$ matrix or $k \times k$ matrix</li><li>So, we have $\mathbf{U}_k \mathbf{\Sigma}_k$ which is $n\times k$ matrix that contains $k$ principle components or transformed features based on variance strength.</li></ul><p>Lower rank (to be investigated later):</p><ul><li>Get the top $k$ rows of $V^T$</li><li>Calculate $\mathbf{X}_k = \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}^T_k $</li><li>This matrix has lower rank (=$k$).</li><li>More on this: <a href=https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation class=external-link target=_blank rel=noopener>https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation</a>.</li></ul><h3 id=6-try-on-a-dataset>6. Try on a dataset
<a class=heading-link href=#6-try-on-a-dataset><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>W. Wolberg. &ldquo;Breast Cancer Wisconsin (Original),&rdquo; UCI Machine Learning Repository, 1990. [Online]. Available: <a href=https://doi.org/10.24432/C5HP4Z class=external-link target=_blank rel=noopener>https://doi.org/10.24432/C5HP4Z</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> ucimlrepo <span style=color:#f92672>import</span> fetch_ucirepo 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#75715e># fetch dataset </span>
</span></span><span style=display:flex><span>breast_cancer_wisconsin_original <span style=color:#f92672>=</span> fetch_ucirepo(id<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>) 
</span></span><span style=display:flex><span><span style=color:#75715e># data (as pandas dataframes) </span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> breast_cancer_wisconsin_original<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>features 
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> breast_cancer_wisconsin_original<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>targets 
</span></span><span style=display:flex><span><span style=color:#75715e># impute `Bare_nuclei` variable (missing data) with mean</span>
</span></span><span style=display:flex><span>X[<span style=color:#e6db74>&#39;Bare_nuclei&#39;</span>] <span style=color:#f92672>=</span> X[<span style=color:#e6db74>&#39;Bare_nuclei&#39;</span>]<span style=color:#f92672>.</span>fillna(X[<span style=color:#e6db74>&#39;Bare_nuclei&#39;</span>]<span style=color:#f92672>.</span>mean())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># standardize and center data</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> X <span style=color:#f92672>-</span> X<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> (X<span style=color:#f92672>-</span>X<span style=color:#f92672>.</span>mean())<span style=color:#f92672>/</span>X<span style=color:#f92672>.</span>std()
</span></span><span style=display:flex><span><span style=color:#75715e># covariance matrix</span>
</span></span><span style=display:flex><span>C <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]) <span style=color:#f92672>*</span> X<span style=color:#f92672>.</span>T <span style=color:#f92672>@</span> X
</span></span><span style=display:flex><span>eigenvalues, eigenvectors <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>eigh(C)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(eigenvalues)
</span></span><span style=display:flex><span>print(eigenvectors)
</span></span></code></pre></div><pre><code>[0.08852979 0.26196357 0.29494031 0.30549327 0.37923046 0.46220462
 0.53807087 0.77554845 5.88114313]
[[-0.00134     0.23175563 -0.01404671  0.27417447  0.06338653  0.10236355
   0.86285736  0.14244407  0.30267028]
 [-0.73455884 -0.45060332  0.19953745  0.09733398 -0.13692903 -0.20358348
  -0.01526165  0.04815858  0.38123865]
 [ 0.66617322 -0.59057132  0.12417806  0.01706757 -0.104332   -0.1718784
   0.03781563  0.08476065  0.37773738]
 [ 0.04718217  0.10055229 -0.12563921  0.67971233  0.01382596  0.46509141
  -0.4251162   0.04390685  0.3327405 ]
 [ 0.0672024   0.4547088  -0.17628399 -0.04258278 -0.67076873 -0.39246704
  -0.10609514 -0.16593569  0.33627742]
 [-0.07693108 -0.06962543 -0.38373562 -0.6040294  -0.12299572  0.53473612
  -0.00911322  0.25461852  0.33338425]
 [ 0.05987162  0.40268926  0.70482078 -0.25250951  0.251001    0.01098134
  -0.1953785   0.22944693  0.34609625]
 [-0.01841871  0.09428063 -0.48658454 -0.05095997  0.649491   -0.44754239
  -0.12547163 -0.02483507  0.33603247]
 [ 0.00708505 -0.04129706  0.13006219 -0.14153277  0.12750515  0.24866722
   0.08851743 -0.90700074  0.22960408]]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Diagonalize the matrix C aka the covariance matrix</span>
</span></span><span style=display:flex><span>D <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(eigenvalues)
</span></span><span style=display:flex><span>V <span style=color:#f92672>=</span> eigenvectors
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># verified: C = VDV^T indeed</span>
</span></span><span style=display:flex><span>diagonalization <span style=color:#f92672>=</span> V <span style=color:#f92672>@</span> D <span style=color:#f92672>@</span> V<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>round(C<span style=color:#f92672>.</span>to_numpy(), <span style=color:#ae81ff>3</span>) <span style=color:#f92672>==</span> np<span style=color:#f92672>.</span>round(diagonalization, <span style=color:#ae81ff>3</span>) <span style=color:#75715e># 3 decimal points</span>
</span></span></code></pre></div><pre><code>array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True,  True,  True,  True,  True]])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># principal components</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sort X based on eigenvalues</span>
</span></span><span style=display:flex><span>sorted_indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argsort(eigenvalues)[::<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#75715e># need to manually sort</span>
</span></span><span style=display:flex><span>eigenvalues_sorted <span style=color:#f92672>=</span> eigenvalues[sorted_indices]
</span></span><span style=display:flex><span>V_sorted <span style=color:#f92672>=</span> V[:, sorted_indices]
</span></span><span style=display:flex><span>column_names_sorted_by_eigenvalues <span style=color:#f92672>=</span> list(X<span style=color:#f92672>.</span>columns[sorted_indices])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pc_covariance <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>to_numpy() <span style=color:#f92672>@</span> V_sorted  
</span></span><span style=display:flex><span><span style=color:#75715e># this new pc have columns that are projections of the original X in the principal directions (?)</span>
</span></span><span style=display:flex><span>pc_covariance <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>round(pc_covariance, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>print(pc_covariance)
</span></span></code></pre></div><pre><code>[[-1.462  0.102  0.576 ...  0.339  0.422  0.004]
 [ 1.463  0.55  -0.3   ... -1.032  0.407 -0.026]
 [-1.584  0.071 -0.039 ...  0.242  0.238 -0.016]
 ...
 [ 3.818  0.167 -0.616 ...  0.691 -0.558  0.07 ]
 [ 2.264  1.106 -0.97  ...  1.706 -0.124 -0.402]
 [ 2.659  1.19  -1.062 ...  1.878 -0.362  0.084]]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># now, let&#39;s if we used the SVD method to get the same principal components or not</span>
</span></span><span style=display:flex><span>U, Sigma, Vt <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>svd(X)
</span></span><span style=display:flex><span><span style=color:#75715e># note that Sigma is already sorted descendingly</span>
</span></span><span style=display:flex><span>Sigma_matrix<span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]))
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>fill_diagonal(Sigma_matrix, Sigma)
</span></span><span style=display:flex><span>pc_svd <span style=color:#f92672>=</span> U <span style=color:#f92672>@</span> Sigma_matrix
</span></span><span style=display:flex><span>pc_svd <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>round(pc_svd, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>pc_svd
</span></span></code></pre></div><pre><code>array([[ 1.462, -0.102,  0.576, ..., -0.339,  0.422,  0.004],
       [-1.463, -0.55 , -0.3  , ...,  1.032,  0.407, -0.026],
       [ 1.584, -0.071, -0.039, ..., -0.242,  0.238, -0.016],
       ...,
       [-3.818, -0.167, -0.616, ..., -0.691, -0.558,  0.07 ],
       [-2.264, -1.106, -0.97 , ..., -1.706, -0.124, -0.402],
       [-2.659, -1.19 , -1.062, ..., -1.878, -0.362,  0.084]])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># validate the both methods result in the same PC</span>
</span></span><span style=display:flex><span>pc_svd <span style=color:#f92672>==</span> pc_covariance
</span></span></code></pre></div><pre><code>array([[False, False,  True, ..., False,  True,  True],
       [False, False,  True, ..., False,  True,  True],
       [False, False,  True, ..., False,  True,  True],
       ...,
       [False, False,  True, ..., False,  True,  True],
       [False, False,  True, ..., False,  True,  True],
       [False, False,  True, ..., False,  True,  True]])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># investigate why there are some differences in signs between two matrices</span>
</span></span><span style=display:flex><span><span style=color:#75715e># TODO</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># use the pc_svd to get fewer features</span>
</span></span><span style=display:flex><span><span style=color:#75715e># original: 9 features, now: suppose we want 4 features</span>
</span></span><span style=display:flex><span>k <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span> 
</span></span><span style=display:flex><span>reconstructed_X <span style=color:#f92672>=</span> pc_svd[:, :k] 
</span></span><span style=display:flex><span>reconstructed_X<span style=color:#f92672>.</span>shape
</span></span></code></pre></div><pre><code>(699, 4)
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># columns picked after PCA</span>
</span></span><span style=display:flex><span>column_names_sorted_by_eigenvalues[:k]
</span></span></code></pre></div><pre><code>['Mitoses', 'Normal_nucleoli', 'Bland_chromatin', 'Bare_nuclei']
</code></pre><h3 id=bonus-pca>Bonus-PCA
<a class=heading-link href=#bonus-pca><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>123</span>)
</span></span><span style=display:flex><span>random_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(random_matrix, columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;feature1&#34;</span>, <span style=color:#e6db74>&#34;feature2&#34;</span>, <span style=color:#e6db74>&#34;feature3&#34;</span>])
</span></span><span style=display:flex><span>print(df)
</span></span></code></pre></div><pre><code>   feature1  feature2  feature3
0 -1.085631  0.997345  0.282978
1 -1.506295 -0.578600  1.651437
2 -2.426679 -0.428913  1.265936
3 -0.866740 -0.678886 -0.094709
4  1.491390 -0.638902 -0.443982
5 -0.434351  2.205930  2.186786
6  1.004054  0.386186  0.737369
7  1.490732 -0.935834  1.175829
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.decomposition <span style=color:#f92672>import</span> PCA
</span></span><span style=display:flex><span>pca <span style=color:#f92672>=</span> PCA(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)  <span style=color:#75715e># suppose we want only 2 PCs</span>
</span></span><span style=display:flex><span>pca<span style=color:#f92672>.</span>fit(df)
</span></span><span style=display:flex><span>eigenvectors <span style=color:#f92672>=</span> pca<span style=color:#f92672>.</span>components_
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;PCs:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, eigenvectors) <span style=color:#75715e># each PC is a row!</span>
</span></span><span style=display:flex><span>print() 
</span></span><span style=display:flex><span>eigenvalues <span style=color:#f92672>=</span> pca<span style=color:#f92672>.</span>explained_variance_
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Eigenvalues:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, eigenvalues)
</span></span></code></pre></div><pre><code>PCs:
 [[ 0.91089251 -0.26571718 -0.31570433]
 [ 0.37939115  0.84018627  0.38749115]]

Eigenvalues:
 [2.35368466 1.27800638]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> mpl_toolkits.mplot3d <span style=color:#f92672>import</span> Axes3D
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transformed_data <span style=color:#f92672>=</span> pca<span style=color:#f92672>.</span>transform(df)
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>111</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>scatter(transformed_data[:, <span style=color:#ae81ff>0</span>], transformed_data[:, <span style=color:#ae81ff>1</span>], s<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#34;PC1&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#34;PC2&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img src=/pca-covariance/fig-pca.png alt></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transformed_data
</span></span></code></pre></div><pre><code>array([[-0.79980352,  0.28440197],
       [-1.19625559, -0.66901683],
       [-1.95269743, -1.04181503],
       [-0.03577698, -1.18725035],
       [ 2.21186851, -0.39434274],
       [-1.12874005,  2.28463639],
       [ 1.12261698,  0.73979447],
       [ 1.77878806, -0.01640788]])
</code></pre><p>This transformed data represents eight observations projected onto two PCs. Each row corresponds to an observation, and each column corresponds to a PC.</p><p>PC1 (more spread out given the center at 0) seems to capture the largest variance, as most of the observations have strong positive or negative values along this component.</p></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2025
Hieu Nguyen
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>