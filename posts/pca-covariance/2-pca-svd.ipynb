{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recently read an interesting explanation for covariance on [StackExchange Cross Validated](https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean). The usage of rectangles and colors are very intuitive to see the directions, means, and strengths given information on covariance and correlation. This got me thinking: I know Principal Component Analysis (PCA) involves a loosely ranking and selection of components based on their variances. Why don't I try implementing this by exploring how the covariance matrix plays a role in this popular dimension reduction and variable selection method?\n",
    "\n",
    "**Note**: If you think I make any mistakes or unclear points, please let me know via email and we can discuss it, I really appreciate your help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick on PCA + SVD\n",
    "\n",
    "SVD = Singular Value Decomposition\n",
    "\n",
    "There seems to be many ways to construct PCA, here I am investigating sequentially the use of covariance matrix and then the use of SVD. A member of the basic linear transforms family, PCA+SVD is feature construction with the goal to transform data into a new coordinate system for dimensionality reduction or class separation. Basically, it reduces the number of features by getting linearly uncorrelated variables (or principal components, or eigenvectors) while maintaining variance. These variables belong to the left singular matrix of the SVD matrix while the variances are the middle matrix or diagonal matrix.\n",
    "\n",
    "There are so many details on this supervised feature selection method (e.g. the relevance of nested subset methods, Gram-Schmidt orthogonalization,...) but I mainly try to see how relevant the covariance matrix is. \n",
    "\n",
    "More info on SVD: https://en.wikipedia.org/wiki/Singular_value_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Covariance matrix\n",
    "\n",
    "Suppose we have a data matrix called $\\mathbf{X}$ that is standardized and centered to the mean 0 (for each column). $\\mathbf{X}$ is $n \\times p$ where $n$ is the number of observations and $p$ is the number of features. Now, the covariance matrix is\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{n-1} (\\mathbf{X}^T-\\bar{\\mathbf{X}^T}) (\\mathbf{X}-\\bar{\\mathbf{X}}) =  \\frac{1}{n-1} (\\mathbf{X}^T) (\\mathbf{X}) \n",
    "$$\n",
    "\n",
    "The question now is why? Why does it look like this? Note that $\\mathbf{X}$ contains multiple features/variables, so doing this means we simultaneously calculate the multiple cases of $Cov(\\mathbf{X}, \\mathbf{Y}) = \\frac{1}{n-1} (\\mathbf{X}-\\bar{\\mathbf{X}})(\\mathbf{Y}-\\bar{\\mathbf{Y}})$. How to interpret the result of the covariance matrix? Each column and each row are the features themselves, the diagonal entries are the \"self\"-covariance while the rest show the covariance value between the intersected variables.\n",
    "\n",
    "$\\mathbf{C}$ is symmetric matrix and so it is diagonalizable: There exists a real orthogonal matrix $\\mathbf{Q}$ and diagonal matrix $\\mathbf{D}$ such that $$\\mathbf{C} = \\mathbf{Q}\\mathbf{D}\\mathbf{Q^T}$$\n",
    "\n",
    "- $\\mathbf{Q}$ contains all the eigenvectors of $\\mathbf{C}$ which are the \"principal directions\", showing the direction in the feature space along which the data varies the most. \n",
    "- $\\mathbf{D}$ represents the eigenvalues or how much variance along that corresponding principal direction. \n",
    "\n",
    "Proof: https://en.wikipedia.org/wiki/Symmetric_matrix\n",
    "\n",
    "### 2. Transformed X\n",
    "\n",
    "Projections of the data on the principal directions are called principal components, or PC scores; these can be seen as newly cretaed or transformed variables. So the transformed data is actually $\\mathbf{X}\\cdot\\mathbf{Q}$\n",
    "\n",
    "### 3. SVD\n",
    "\n",
    "SVD factorizes $\\mathbf{X}$ (not just square matrices) into three matrices ([proof](https://gregorygundersen.com/blog/2018/12/20/svd-proof/)):\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{U}$ is a matrix of left singular vectors (related to the rows of $\\mathbf{X}$),\n",
    "- $\\mathbf{\\Sigma}$ is a diagonal matrix of singular values (related to the square roots of the eigenvalues), the diagnonal entries, $\\sigma_i = \\Sigma_{ii}$ are uniquely defined by $X$ and known as the singular values of $X$. These are non-negative real numbers. by convention the diagonal elements are in descending order.\n",
    "- $\\mathbf{V}^T$ is the transpose of a matrix of right singular vectors (related to the columns of $\\mathbf{X}$).\n",
    "\n",
    "### 4. Covariance matrix and SVD\n",
    "\n",
    "Note that \n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{n-1}(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T)^T \\cdot (\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T) = \\mathbf{V} \\frac{\\mathbf{\\Sigma}^2}{n-1} \\mathbf{V}^T\n",
    "$$\n",
    "\n",
    "From the breakdown in part (1), it seems that $\\mathbf{Q}$ is actually $\\mathbf{V}$! So $V$ contains eigenvectors or principal directions of the covariance matrix $\\mathbf{C}$. And $\\frac{\\mathbf{\\Sigma}^2}{n-1}=\\mathbf{D}$ represents how strong the variance of the corresponding directions are (the diagonal entries are eigenvalues). And the transformed data is $\\mathbf{X}\\cdot\\mathbf{V} = (n\\times p) \\cdot (p\\times p)$.\n",
    "\n",
    "The $i$-th column of $XV$: This column represents the values of the $i$-th principal component for each sample. It is the transformed version of the original data in the direction of the $i$-th eigenvector.\n",
    "\n",
    "Also, notice that $XV=U\\Sigma V^T V=U\\Sigma$.\n",
    "\n",
    "### 5. Reduce dimensionality\n",
    "\n",
    "Now, we would to have fewer features, i.e. $k<p$. \n",
    "\n",
    "- Get the initial $k$ columns from $\\mathbf{U}$ or $n \\times k$ matrix\n",
    "- Get the initial $k$ columns and $k$ rows from $\\mathbf{\\Sigma}$ matrix or $k \\times k$ matrix\n",
    "- So, we have $\\mathbf{U}_k \\mathbf{\\Sigma}_k$ which is $n\\times k$ matrix that contains $k$ principle components or transformed features based on variance strength.\n",
    "\n",
    "Lower rank (to be investigated later):\n",
    "\n",
    "- Get the top $k$ rows of $V^T$\n",
    "- Calculate $\\mathbf{X}_k = \\mathbf{U}_k \\mathbf{\\Sigma}_k \\mathbf{V}^T_k $\n",
    "- This matrix has lower rank (=$k$). \n",
    "- More on this: https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on a dataset\n",
    "\n",
    "W. Wolberg. \"Breast Cancer Wisconsin (Original),\" UCI Machine Learning Repository, 1990. [Online]. Available: https://doi.org/10.24432/C5HP4Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import numpy as np\n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_original.data.features \n",
    "y = breast_cancer_wisconsin_original.data.targets \n",
    "\n",
    "# impute `Bare_nuclei` variable (missing data) with mean\n",
    "X['Bare_nuclei'] = X['Bare_nuclei'].fillna(X['Bare_nuclei'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " Index(['Clump_thickness', 'Uniformity_of_cell_size',\n",
       "        'Uniformity_of_cell_shape', 'Marginal_adhesion',\n",
       "        'Single_epithelial_cell_size', 'Bare_nuclei', 'Bland_chromatin',\n",
       "        'Normal_nucleoli', 'Mitoses'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count features\n",
    "len(X.columns), X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize and center data\n",
    "X = X - X.mean()\n",
    "X = (X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix\n",
    "C = (1/X.shape[0]) * X.T @ X\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08852979 0.26196357 0.29494031 0.30549327 0.37923046 0.46220462\n",
      " 0.53807087 0.77554845 5.88114313]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00134   ,  0.23175563, -0.01404671,  0.27417447,  0.06338653,\n",
       "         0.10236355,  0.86285736,  0.14244407,  0.30267028],\n",
       "       [-0.73455884, -0.45060332,  0.19953745,  0.09733398, -0.13692903,\n",
       "        -0.20358348, -0.01526165,  0.04815858,  0.38123865],\n",
       "       [ 0.66617322, -0.59057132,  0.12417806,  0.01706757, -0.104332  ,\n",
       "        -0.1718784 ,  0.03781563,  0.08476065,  0.37773738],\n",
       "       [ 0.04718217,  0.10055229, -0.12563921,  0.67971233,  0.01382596,\n",
       "         0.46509141, -0.4251162 ,  0.04390685,  0.3327405 ],\n",
       "       [ 0.0672024 ,  0.4547088 , -0.17628399, -0.04258278, -0.67076873,\n",
       "        -0.39246704, -0.10609514, -0.16593569,  0.33627742],\n",
       "       [-0.07693108, -0.06962543, -0.38373562, -0.6040294 , -0.12299572,\n",
       "         0.53473612, -0.00911322,  0.25461852,  0.33338425],\n",
       "       [ 0.05987162,  0.40268926,  0.70482078, -0.25250951,  0.251001  ,\n",
       "         0.01098134, -0.1953785 ,  0.22944693,  0.34609625],\n",
       "       [-0.01841871,  0.09428063, -0.48658454, -0.05095997,  0.649491  ,\n",
       "        -0.44754239, -0.12547163, -0.02483507,  0.33603247],\n",
       "       [ 0.00708505, -0.04129706,  0.13006219, -0.14153277,  0.12750515,\n",
       "         0.24866722,  0.08851743, -0.90700074,  0.22960408]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eigenvalues)\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diagonalize the matrix C aka the covariance matrix\n",
    "D = np.diag(eigenvalues)\n",
    "V = eigenvectors\n",
    "\n",
    "# verified: C = VDV^T indeed\n",
    "diagonalization = V @ D @ V.T\n",
    "np.round(C.to_numpy(), 3) == np.round(diagonalization, 3) # 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.462  0.102  0.576 ...  0.339  0.422  0.004]\n",
      " [ 1.463  0.55  -0.3   ... -1.032  0.407 -0.026]\n",
      " [-1.584  0.071 -0.039 ...  0.242  0.238 -0.016]\n",
      " ...\n",
      " [ 3.818  0.167 -0.616 ...  0.691 -0.558  0.07 ]\n",
      " [ 2.264  1.106 -0.97  ...  1.706 -0.124 -0.402]\n",
      " [ 2.659  1.19  -1.062 ...  1.878 -0.362  0.084]]\n"
     ]
    }
   ],
   "source": [
    "# principal components\n",
    "\n",
    "# sort X based on eigenvalues\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1] # need to manually sort\n",
    "eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "V_sorted = V[:, sorted_indices]\n",
    "column_names_sorted_by_eigenvalues = list(X.columns[sorted_indices])\n",
    "\n",
    "pc_covariance = X.to_numpy() @ V_sorted  \n",
    "# this new pc have columns that are projections of the original X in the principal directions (?)\n",
    "pc_covariance = np.round(pc_covariance, 3)\n",
    "print(pc_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.462, -0.102,  0.576, ..., -0.339,  0.422,  0.004],\n",
       "       [-1.463, -0.55 , -0.3  , ...,  1.032,  0.407, -0.026],\n",
       "       [ 1.584, -0.071, -0.039, ..., -0.242,  0.238, -0.016],\n",
       "       ...,\n",
       "       [-3.818, -0.167, -0.616, ..., -0.691, -0.558,  0.07 ],\n",
       "       [-2.264, -1.106, -0.97 , ..., -1.706, -0.124, -0.402],\n",
       "       [-2.659, -1.19 , -1.062, ..., -1.878, -0.362,  0.084]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, let's if we used the SVD method to get the same principal components or not\n",
    "U, Sigma, Vt = np.linalg.svd(X)\n",
    "# note that Sigma is already sorted descendingly\n",
    "\n",
    "Sigma_matrix= np.zeros((X.shape[0], X.shape[1]))\n",
    "np.fill_diagonal(Sigma_matrix, Sigma)\n",
    "\n",
    "pc_svd = U @ Sigma_matrix\n",
    "pc_svd = np.round(pc_svd, 3)\n",
    "pc_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True, ..., False,  True,  True],\n",
       "       [False, False,  True, ..., False,  True,  True],\n",
       "       [False, False,  True, ..., False,  True,  True],\n",
       "       ...,\n",
       "       [False, False,  True, ..., False,  True,  True],\n",
       "       [False, False,  True, ..., False,  True,  True],\n",
       "       [False, False,  True, ..., False,  True,  True]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the both methods result in the same PC\n",
    "pc_svd == pc_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate why there are some differences in signs between two matrices\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 4)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the pc_svd to get fewer features\n",
    "# original: 9 features, now: suppose we want 4 features\n",
    "# k = 4\n",
    "\n",
    "k = 4 \n",
    "reconstructed_X = pc_svd[:, :k] \n",
    "reconstructed_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mitoses', 'Normal_nucleoli', 'Bland_chromatin', 'Bare_nuclei']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns picked after PCA\n",
    "column_names_sorted_by_eigenvalues[:k]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
