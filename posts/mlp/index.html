<!doctype html><html lang=en><head><title>Multilayer perceptron · Home
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Hieu Nguyen"><meta name=description content="Overview Link to heading Goal: Explain how multilayer perceptron model is trained.
Half of the battle is the configurations and notations:
Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned. There are $m$ instances in the data, 625 features, and 10 labels. The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Multilayer perceptron"><meta name=twitter:description content="Overview Link to heading Goal: Explain how multilayer perceptron model is trained.
Half of the battle is the configurations and notations:
Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned. There are $m$ instances in the data, 625 features, and 10 labels. The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i."><meta property="og:url" content="https://quanghieu31.github.io/posts/mlp/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Multilayer perceptron"><meta property="og:description" content="Overview Link to heading Goal: Explain how multilayer perceptron model is trained.
Half of the battle is the configurations and notations:
Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned. There are $m$ instances in the data, 625 features, and 10 labels. The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-09T12:00:00+00:00"><meta property="article:modified_time" content="2024-05-09T12:00:00+00:00"><link rel=canonical href=https://quanghieu31.github.io/posts/mlp/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.135e22c97ff685fe983fc60048e309ced8f00d8d38f536aa67dba8a13a03dfa4.css integrity="sha256-E14iyX/2hf6YP8YASOMJztjwDY049TaqZ9uooToD36Q=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/custom.min.82caf190d01cd5b664127e60afa93081244659afd1e288eda818c81fb0e63981.css integrity="sha256-gsrxkNAc1bZkEn5gr6kwgSRGWa/R4ojtqBjIH7DmOYE=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon.ico sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Hieu Nguyen"><meta name=description content="Overview Link to heading Goal: Explain how multilayer perceptron model is trained.
Half of the battle is the configurations and notations:
Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned. There are $m$ instances in the data, 625 features, and 10 labels. The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Multilayer perceptron"><meta name=twitter:description content="Overview Link to heading Goal: Explain how multilayer perceptron model is trained.
Half of the battle is the configurations and notations:
Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned. There are $m$ instances in the data, 625 features, and 10 labels. The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i."><meta property="og:url" content="https://quanghieu31.github.io/posts/mlp/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Multilayer perceptron"><meta property="og:description" content="Overview Link to heading Goal: Explain how multilayer perceptron model is trained.
Half of the battle is the configurations and notations:
Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned. There are $m$ instances in the data, 625 features, and 10 labels. The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-09T12:00:00+00:00"><meta property="article:modified_time" content="2024-05-09T12:00:00+00:00"><link href="https://fonts.googleapis.com/css2?family=Source+Sans+3&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/ style=font-size:18px>Home
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list style=font-size:18px><li class=navigation-item><a class=navigation-link href=/about/ style=font-size:18px>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/ style=font-size:18px>Posts</a></li></ul></section></nav><div class=content><section class="container post"><article><style>.content{max-width:1000px;margin:40px auto 10px;padding:0 15px;font-size:16px;line-height:1.8}</style><header><div class=post-title><h1 class=title><a class=title-link href=https://quanghieu31.github.io/posts/mlp/ style=font-size:22px>Multilayer perceptron</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2024-05-09T12:00:00Z>May 9, 2024
</time></span><span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
3-minute read</span></div></div></header><div class=post-content><h1 id=overview>Overview
<a class=heading-link href=#overview><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Goal: Explain how multilayer perceptron model is trained.</p><p>Half of the battle is the configurations and notations:</p><ul><li>Suppose we want to classify 25x25 images and have 10 output labels $(\textbf{y})$. The data for training are labeled and cleaned.</li><li>There are $m$ instances in the data, 625 features, and 10 labels.</li><li>The data can be represented as a $m \times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i.e. some $x_0$). That is, the input layer is a $1 \times 625$. As a convention (I believe), we should transpose this matrix and get the $625 \times 1$ input layer. Then, the output layer is the activations of 10 labels which is a $10 \times 1$ matrix. Output layer notation: $\textbf{a}^{(3)} = [ [a_0^{(3)}], [a_1^{(3)}], \ldots, [a_{9}^{(3)}]]$. To be more general, let $j$ index the output matrix&rsquo;s elements.</li><li>Choose 2 hidden layers: the first layer has 5 neurons and uses ReLU as the activation function and the second layer has 8 neurons and uses a softmax function. The number of neurons is arbitrarily selected. Respective layer notations: $\textbf{a}^{(1)} = [[a_0^{(1)}], [a_1^{(1)}], \ldots, [a_4^{(1)}]]$ and $\textbf{a}^{(2)} = [[a_0^{(2)}], [a_1^{(2)}], \ldots, [a_7^{(2)} ]]$. To be more general, let $k$ index the layer (2) matrix&rsquo;s elements and let $h$ index the layer (1) matrix&rsquo;s elements.</li><li>Choose the Mean Squared Error (MSE) loss function. For one single training example $x_i$, $ℒ_i = \sum_{j=0}^{9} (a_j^{(3)} - y_j)^{2}$. Then, the overall loss function for all training examples is $ℒ = \frac{1}{m} \sum_{i=0}^{m-1} \mathscr{L_i}$.</li><li>Choose the Stochastic Gradient Descent technique for mini-batching and randomization optimization. Iterate through each training example (or randomly selected mini-batches of examples) and compute the gradient of the loss function with respect to the $W$ and $b$ parameters using that example or mini-batch. Update the parameters with the computed gradients and a learning rate. Repeat the process for multiple iterations (epochs) until convergence or a stopping criterion is met.</li><li>Last but not least: weight and bias parameters. They appear in each of the layers except for input layer.<ul><li>$\textbf{W}^{(1)}$ and $\textbf{b}^{(1)}$ denote the weights and biases of layer (1) in which $\textbf{W}^{(1)}$ is a $5 \times 625$ matrix and $\textbf{b}^{(1)}$ is a $5 \times 1$ matrix.</li><li>$\textbf{W}^{(2)}$ and $\textbf{b}^{(2)}$ denote the weights and biases of layer (2) in which $\textbf{W}^{(2)}$ is a $8 \times 5$ matrix and $\textbf{b}^{(2)}$ is a $8 \times 1$ matrix.</li><li>$\textbf{W}^{(3)}$ and $\textbf{b}^{(3)}$ denote the weights and biases of output layer in which $\textbf{W}^{(3)}$ is a $10 \times 8$ matrix and $\textbf{b}^{(3)}$ is a $10 \times 1$ matrix.</li></ul></li></ul><p>This is my attempt to draw a neural network for one single training example:</p><p><img src=/mlp/nn.png alt></p><h1 id=additional-notations>Additional notations
<a class=heading-link href=#additional-notations><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><ul><li>Weighted sum $z$</li><li>ReLU and softmax</li><li>SGD, regularization</li></ul><h1 id=algorithms>Algorithms
<a class=heading-link href=#algorithms><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Repeating over epochs:</p><ul><li>Feed forward</li><li>Back prop</li><li>Update</li></ul><h1 id=back-propagation>Back propagation
<a class=heading-link href=#back-propagation><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h1 id=code-heart-disease-classification>Code: Heart disease classification
<a class=heading-link href=#code-heart-disease-classification><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Information on the data <a href=https://archive.ics.uci.edu/dataset/45/heart+disease. class=external-link target=_blank rel=noopener>here</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># libraries</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> ucimlrepo <span style=color:#f92672>import</span> fetch_ucirepo 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># fetch dataset </span>
</span></span><span style=display:flex><span>heart_disease <span style=color:#f92672>=</span> fetch_ucirepo(id<span style=color:#f92672>=</span><span style=color:#ae81ff>45</span>) 
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#75715e># data (as pandas dataframes) </span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> heart_disease<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>features 
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> heart_disease<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>targets 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># variable information </span>
</span></span><span style=display:flex><span>heart_disease<span style=color:#f92672>.</span>variables
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>heart_disease[<span style=color:#e6db74>&#34;data&#34;</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y
</span></span></code></pre></div><h1 id=references>References:
<a class=heading-link href=#references><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Janosi,Andras, Steinbrunn,William, Pfisterer,Matthias, and Detrano,Robert. (1988). Heart Disease. UCI Machine Learning Repository. <a href=https://doi.org/10.24432/C52P4X class=external-link target=_blank rel=noopener>https://doi.org/10.24432/C52P4X</a>.</p></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2024
Hieu Nguyen
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>