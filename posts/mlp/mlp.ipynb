{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Goal: Explain how multilayer perceptron model is trained. \n",
    "\n",
    "Half of the battle is the configurations and notations:\n",
    "- Suppose we want to classify 25x25 images and have 10 output labels $(\\textbf{y})$. The data for training are labeled and cleaned. \n",
    "- There are $m$ instances in the data, 625 features, and 10 labels. \n",
    "- The data can be represented as a $m \\times 625$ matrix or tabular-type. For simplicity, we will work with each single training example (i.e. some $x_0$). That is, the input layer is a $1 \\times 625$. As a convention (I believe), we should transpose this matrix and get the $625 \\times 1$ input layer. Then, the output layer is the activations of 10 labels which is a $10 \\times 1$ matrix. Output layer notation: $\\textbf{a}^{(3)} = [ [a_0^{(3)}], [a_1^{(3)}], \\ldots, [a_{9}^{(3)}]]$. To be more general, let $j$ index the output matrix's elements. \n",
    "- Choose 2 hidden layers: the first layer has 5 neurons and uses ReLU as the activation function and the second layer has 8 neurons and uses a softmax function. The number of neurons is arbitrarily selected. Respective layer notations: $\\textbf{a}^{(1)} = [[a_0^{(1)}], [a_1^{(1)}], \\ldots, [a_4^{(1)}]]$ and $\\textbf{a}^{(2)} = [[a_0^{(2)}], [a_1^{(2)}], \\ldots, [a_7^{(2)} ]]$. To be more general, let $k$ index the layer (2) matrix's elements and let $h$ index the layer (1) matrix's elements. \n",
    "- Choose the Mean Squared Error (MSE) loss function. For one single training example $x_i$, $ℒ_i = \\sum_{j=0}^{9} (a_j^{(3)} - y_j)^{2}$. Then, the overall loss function for all training examples is $ℒ = \\frac{1}{m} \\sum_{i=0}^{m-1} \\mathscr{L_i}$.\n",
    "- Choose the Stochastic Gradient Descent technique for mini-batching and randomization optimization. Iterate through each training example (or randomly selected mini-batches of examples) and compute the gradient of the loss function with respect to the $W$ and $b$ parameters using that example or mini-batch. Update the parameters with the computed gradients and a learning rate. Repeat the process for multiple iterations (epochs) until convergence or a stopping criterion is met.\n",
    "- Last but not least: weight and bias parameters. They appear in each of the layers except for input layer. \n",
    "    - $\\textbf{W}^{(1)}$ and $\\textbf{b}^{(1)}$ denote the weights and biases of layer (1) in which $\\textbf{W}^{(1)}$ is a $5 \\times 625$ matrix and $\\textbf{b}^{(1)}$ is a $5 \\times 1$ matrix. \n",
    "    - $\\textbf{W}^{(2)}$ and $\\textbf{b}^{(2)}$ denote the weights and biases of layer (2) in which $\\textbf{W}^{(2)}$ is a $8 \\times 5$ matrix and $\\textbf{b}^{(2)}$ is a $8 \\times 1$ matrix. \n",
    "    - $\\textbf{W}^{(3)}$ and $\\textbf{b}^{(3)}$ denote the weights and biases of output layer in which $\\textbf{W}^{(3)}$ is a $10 \\times 8$ matrix and $\\textbf{b}^{(3)}$ is a $10 \\times 1$ matrix.   \n",
    "\n",
    "This is my attempt to draw a neural network for one single training example:\n",
    "\n",
    "![a](figs/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional notations\n",
    "\n",
    "- Weighted sum $z$\n",
    "- ReLU and softmax\n",
    "- SGD, regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms \n",
    "\n",
    "Repeating over epochs:\n",
    "- Feed forward\n",
    "- Back prop\n",
    "- Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: Heart disease classification\n",
    "\n",
    "Information on the data [here](https://archive.ics.uci.edu/dataset/45/heart+disease.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "\n",
    "# variable information \n",
    "heart_disease.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"data\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "Janosi,Andras, Steinbrunn,William, Pfisterer,Matthias, and Detrano,Robert. (1988). Heart Disease. UCI Machine Learning Repository. https://doi.org/10.24432/C52P4X."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
